# 交叉熵损失函数

[TOC]

## 基本损失函数

损失函数作用

1. 计算出实际输出和目标之间的差距
2. 为我们更新输出提供依据（反向传播）

$$
output:  10,10,20 \\
target :  30,20,50  \\ 
loss=|(30-10)|+|(20-10)|+\mid ( 50-10) \mid=70\\  
L1loss=70 / 3=23.33
$$



## 交叉熵

![image-20220603115920208](https://yzfzzz.oss-cn-shenzhen.aliyuncs.com/image/202206031159295.png)

两个不同的模型比较，需要熵作为中介。比如，黄金和白银比较价值，需要把他们都换算为美元，才能对比



### 1.信息量

1. 不同的信息，含有不同的信息量，假设下列对阵表中阿根廷的夺冠概率是1/8，A同学告诉我阿根廷夺冠了，那么这个信息量就很大了（因为它包括了阿根廷进了四强，决赛）；B同学告诉我阿根廷进决赛了，那么这个信息量就较小。

   <img src="https://yzfzzz.oss-cn-shenzhen.aliyuncs.com/image/202206042246234.png" alt="image-20220604224650165" style="zoom:67%;" />

2. 假设f(x):= 信息量（:=是定义符），x是信息

   - $f（阿根廷夺冠）= f（阿根廷进决赛）+ f（阿根廷赢了决赛）$

   
   因为事件越不确定，则其包含的信息量就越多,所以`自变量又可以变为事件的概率`
   
   则有：
   
   $f（1/8）= f（1/4）+ f(1/2)$
   
   同时，也必须满足 
   
   - $P（阿根廷夺冠）= P(阿根廷进决赛)*P（阿根廷赢得了决赛）$
   
   所以 
   
   🚀$f(P(阿根廷夺冠)*P(阿根廷赢得了决赛))=f(P(阿根廷进决赛))+ f(P(阿根廷赢得了决赛))$
   
   所以，用表达式中肯定有🌻log
   
   又因为事件发生概率和信息量成反比，所以有 -log
   
   
   
   

<img src="https://yzfzzz.oss-cn-shenzhen.aliyuncs.com/image/202206042256684.png" alt="image-20220604225652608" style="zoom:67%;" />



### 2.熵

🔥熵：一个事件，从原来的不确定到完全确定，有多大的`难度`。而信息量的期望，就是熵H(P)

<img src="https://yzfzzz.oss-cn-shenzhen.aliyuncs.com/image/202206042245845.png" alt="image-20220604224535714" style="zoom:67%;" />
$$
\begin{array}{c}
H(P):=E\left(P_{f}\right) \\
=\sum_{i=1}^{m} p_{i} \cdot f\left(p_{i}\right)=\sum_{i=1}^{m} p_{i}\left(-\log _{2} p_{i}\right)=-\sum_{i=1}^{m} p_{i} \cdot \log _{2} p_{i}
\end{array}
$$


🌈$P_f$是总信息量，$f(p_i)$是该事件的信息量，$p_i$是该事件发生的概率

交叉熵越小，两个模型就越接近



### 3.相对熵（KL散度）

$f_Q(q_i)$表示Q系统的信息量；$f_P(p_i)$是P系统的信息量

$D_KL(P||Q)$表示两个系统的相对熵，或者说KL散度

$D_KL(P||Q)$以P为基准，去考虑P、Q相差多少

$D_KL(Q||P)$表示以Q为基准

- $\sum_{i=1}^{m} p_{i} \cdot\left(f_{Q}\left(q_{i}\right)-f_{P}\left(p_{i}\right)\right) \\$


$f_{Q}\left(q_{i}\right)-f_{P}\left(p_{i}\right)$表示某一事件，在Q系统的信息量，减去P系统的信息量



![image-20220604231434495](https://yzfzzz.oss-cn-shenzhen.aliyuncs.com/image/202206042314553.png)


$$
\begin{array}{l}
\boldsymbol{D}_{\boldsymbol{K} \boldsymbol{L}}(\boldsymbol{P} \| \boldsymbol{Q}) \\
:=\sum_{i=1}^{m} p_{i} \cdot\left(f_{Q}\left(q_{i}\right)-f_{P}\left(p_{i}\right)\right) \\
=\sum_{i=1}^{m} p_{i} \cdot\left(\left(-\log _{2} q_{i}\right)-\left(-\log _{2} p_{i}\right)\right) \\
=\sum_{i=1}^{m} p_{i} \cdot\left(-\log _{2} q_{i}\right)-\sum_{m i=1}^{m} p_{i} \cdot\left(-\log _{2} p_{i}\right)
\end{array}
$$

$\sum_{m i=1}^{m} p_{i} \cdot(-\log _{2} p_{i})$:P的熵，因为我们把P定做基准了，所以看散度时，只需要考虑$\sum_{i=1}^{m} p_{i} \cdot\left(-\log _{2} q_{i}\right)$,这一部分，就是`交叉熵`了

<img src="https://yzfzzz.oss-cn-shenzhen.aliyuncs.com/image/202206092043871.png" alt="202206042318786" style="zoom:67%;" />

1. 二分类问题
   
   交叉熵要包含所有可能的结果，而二分类的结果为：是/否，所以要有$(1-x_{i}) \cdot \log _{2}\left(1-y_{i}\right)$
   $$
   \begin{array}{ll}
   \boldsymbol{H}(\boldsymbol{P}, \boldsymbol{Q})
   & =-\sum_{i=1}^{n}\left(x_{i} \cdot \log _{2} y_{i}+\left(1-x_{i}\right) \cdot \log _{2}\left(1-y_{i}\right)\right)
   \end{array}
   $$
   
2. 多分类问题

$$
\boldsymbol{H}(\boldsymbol{P}, \boldsymbol{Q})
& =\sum_{i=1}^{m} p_{i} \cdot\left(-\log _{2} q_{i}\right) \\
$$

注意：python中log的底是e，即ln

![image-20220609201335688](https://yzfzzz.oss-cn-shenzhen.aliyuncs.com/image/202206092013206.png)



### 4.交叉熵

![image-20220603115920208](https://yzfzzz.oss-cn-shenzhen.aliyuncs.com/image/202206031159295.png)

pytorch中的交叉熵有点不太一样，它是以`softmax`函数作为事件的概率

$w_c$是权重

理论很难，使用起来确很简单，就一句代码的事~😎

```python
loss_fn = nn.CrossEntropyLoss() # 交叉熵损失
```

